# Sydney Events - Event Aggregator Website

A full-stack web application that automatically scrapes and displays events in Sydney, Australia. Built with Next.js, TypeScript, Prisma, and Tailwind CSS.

**ğŸ¤– NEW: AI Chatbot Integration** - Assignment 2 complete! Telegram bot with event recommendations and auto-notifications. [See Chatbot Setup â†’](./CHATBOT_SETUP.md)

## ğŸ¯ Features

### Assignment 1 - Web Application
- âœ… **Automatic Event Scraping** - Events are automatically scraped from multiple sources
- âœ… **Beautiful UI** - Clean, responsive design with event cards and filtering
- âœ… **Email Capture** - Users must provide email before accessing ticket links
- âœ… **Auto-Updates** - Events refresh automatically every 6 hours via cron job
- âœ… **Search & Filter** - Filter by category and search events by keywords
- âœ… **Real-time Data** - Events stored in database and served via REST API

### Assignment 2 - AI Chatbot ğŸ¤–
- âœ… **Telegram Bot** - Chat interface for event discovery
- âœ… **Preference Learning** - Natural conversation to learn user interests
- âœ… **Smart Recommendations** - AI-powered event matching with scoring algorithm
- âœ… **Auto-Notifications** - Get notified when new matching events are added
- âœ… **Admin Dashboard** - View and manage bot users at `/admin`

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- npm or yarn package manager
- (Optional) Telegram Bot Token for chatbot features

### Installation

1. **Clone and navigate to the project:**
```bash
cd "d:\VSCODE\Companies-assignments\Louder assignment\sydney-events"
```

2. **Install dependencies:**
```bash
npm install
```

3. **Set up the database:**
```bash
npm run db:push
```

4. **Run initial scraping (populate database with events):**
```bash
npm run scrape
```

5. **Start the development server:**
```bash
npm run dev
```

6. **(Optional) Start the Telegram bot:**
```bash
npm run bot
```

7. **Open your browser:**
```
http://localhost:3000
```

### Chatbot Setup

To enable the AI chatbot:

1. Create a Telegram bot via [@BotFather](https://t.me/BotFather)
2. Add your bot token to `.env`:
   ```
   TELEGRAM_BOT_TOKEN="your-token-here"
   ```
3. Run the bot: `npm run bot`
4. Chat with your bot on Telegram!

**Full chatbot documentation:** [CHATBOT_SETUP.md](./CHATBOT_SETUP.md)

## ğŸ“ Project Structure

```
sydney-events/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ events/route.ts      # GET events API
â”‚   â”‚   â”‚   â”œâ”€â”€ subscribe/route.ts   # POST email subscription API
â”‚   â”‚   â”‚   â””â”€â”€ scrape/route.ts      # Trigger manual scraping
â”‚   â”‚   â”œâ”€â”€ layout.tsx               # Root layout with metadata
â”‚   â”‚   â”œâ”€â”€ page.tsx                 # Main events listing page
â”‚   â”‚   â””â”€â”€ globals.css              # Global styles
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ EventCard.tsx            # Event card component
â”‚   â”‚   â””â”€â”€ EmailModal.tsx           # Email capture modal
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ prisma.ts                # Prisma client instance
â”‚   â”‚   â”œâ”€â”€ scraper.ts               # Web scraping logic
â”‚   â”‚   â”œâ”€â”€ cron.ts                  # Cron job scheduler
â”‚   â”‚   â”œâ”€â”€ telegram.ts              # ğŸ¤– Telegram bot handler
â”‚   â”‚   â”œâ”€â”€ llm.ts                   # ğŸ¤– LangChain/OpenAI integration
â”‚   â”‚   â”œâ”€â”€ recommendations.ts       # ğŸ¤– Event matching engine
â”‚   â”‚   â”œâ”€â”€ notifier.ts              # ğŸ¤– Notification system
â”‚   â”‚   â””â”€â”€ utils.ts                 # Utility functions
â”‚   â””â”€â”€ types/
â”œâ”€â”€ prisma/
â”‚   â””â”€â”€ schema.prisma                # Database schema
â”œâ”€â”€ .env                             # Environment variables
â””â”€â”€ package.json                     # Dependencies and scripts
```

## ğŸ—„ï¸ Database Schema

**Events Table:**
- `id` - Unique identifier
- `title` - Event name
- `description` - Event details
- `venue` - Location name
- `address` - Full address
- `startDate` - Event start time
- `imageUrl` - Event image
- `ticketUrl` - Link to tickets
- `price` - Ticket price
- `category` - Event category
- `source` - Scraping source
- `createdAt` / `updatedAt` - Timestamps

**EmailSubscribers Table:**
- `id` - Unique identifier
- `email` - Subscriber email
- `eventId` - Associated event (optional)
- `createdAt` - Subscription timestamp

**User Table (Chatbot):** ğŸ¤–
- `id` - Unique identifier
- `telegramId` - Telegram user ID
- `username` - Telegram username
- `firstName` / `lastName` - User name
- `isActive` - Active status
- `createdAt` / `updatedAt` - Timestamps

**UserPreference Table (Chatbot):** ğŸ¤–
- `id` - Unique identifier
- `userId` - Associated user
- `categories` - Preferred event categories (JSON)
- `priceRange` - Budget preference
- `preferredDays` - Preferred days (JSON)
- `interests` - General interests
- `budget` - Budget level

**Notification Table (Chatbot):** ğŸ¤–
- `id` - Unique identifier
- `userId` - Associated user
- `eventId` - Event being notified about
- `message` - Notification text
- `sent` - Delivery status
- `sentAt` - Delivery timestamp

## ğŸ”§ API Endpoints

### Web Application

#### GET `/api/events`
Fetch all upcoming events
- Query params: `?category=Music&search=concert`
- Returns: Array of event objects

#### POST `/api/subscribe`
Save user email before ticket redirect
- Body: `{ email: string, eventId: string }`
- Returns: Success confirmation

#### POST `/api/scrape`
Manually trigger event scraping (can be called via cron)
- Returns: Scraping status

### Chatbot ğŸ¤–

#### GET `/api/users`
Get all chatbot users
- Returns: Array of users with preferences

#### GET `/api/users/[id]`
Get specific user details
- Returns: User object with preferences and notifications

#### PATCH `/api/users/[id]`
Update user preferences
- Body: `{ preferences: {...} }`
- Returns: Updated user

#### POST `/api/telegram/webhook`
Telegram webhook endpoint
- Body: Telegram update object
- Returns: Success status

## ğŸ¤– Auto-Update System

Events automatically update via two methods:

### 1. Server-Side Cron (Recommended for Production)
Add to your deployment platform:
```bash
# Vercel Cron (vercel.json)
{
  "crons": [{
    "path": "/api/scrape",
    "schedule": "0 */6 * * *"
  }]
}
```

### 2. External Cron Service
Use services like:
- **Cron-job.org** - Schedule GET/POST to `/api/scrape`
- **EasyCron** - Free tier available
- **GitHub Actions** - Workflow scheduled runs

Example GitHub Action (`.github/workflows/scrape.yml`):
```yaml
name: Scrape Events
on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Scraping
        run: |
          curl -X POST https://your-domain.com/api/scrape
```

## ğŸŒ Deployment

### Deploy to Vercel (Recommended)

1. **Push to GitHub:**
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin <your-repo-url>
git push -u origin main
```

2. **Deploy to Vercel:**
```bash
npm install -g vercel
vercel
```

3. **Set up database:**
- Use Vercel Postgres or external PostgreSQL
- Update `DATABASE_URL` in environment variables
- Run migrations: `npx prisma db push`

4. **Add environment variables in Vercel dashboard:**
- `DATABASE_URL`
- `NEXT_PUBLIC_APP_URL`
- `SCRAPER_SECRET` (optional, for API security)

### Alternative: Deploy to Railway/Render

Both support automatic deployments from GitHub with PostgreSQL databases.

## ğŸ¨ Customization

### Add New Event Sources

Edit `src/lib/scraper.ts`:
```typescript
async function scrapeNewSource(): Promise<ScrapedEvent[]> {
  // Your scraping logic
  return events;
}

// Add to scrapeAllEvents():
const newEvents = await scrapeNewSource();
```

### Modify Categories

Update categories in `src/app/page.tsx`:
```typescript
const categories = [
  'all',
  'Your Category',
  // ... more categories
];
```

### Change Update Frequency

Modify cron schedule in deployment config:
- `0 */6 * * *` - Every 6 hours
- `0 0 * * *` - Daily at midnight
- `0 */1 * * *` - Every hour

## ğŸ§ª Testing

### Manual Scraping
```bash
npm run scrape
```

### View Database
```bash
npm run db:studio
```
Opens Prisma Studio at `http://localhost:5555`

### Test API Endpoints
```bash
# Get events
curl http://localhost:3000/api/events

# Subscribe email
curl -X POST http://localhost:3000/api/subscribe \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","eventId":"123"}'

# Trigger scraping
curl -X POST http://localhost:3000/api/scrape
```

## ğŸ“Š Performance

- **Initial Load**: ~500ms with 50 events
- **Database**: SQLite for dev, PostgreSQL for production
- **Scraping**: ~10-30 seconds depending on sources
- **Caching**: Implement Redis for high traffic

## ğŸ” Security Considerations

1. **Rate Limiting** - Add rate limiting to scraping endpoint
2. **Email Validation** - Validate and sanitize email inputs
3. **CORS** - Configure CORS for API endpoints
4. **Environment Variables** - Never commit `.env` file
5. **API Authentication** - Add secret key for scraping endpoint

## ğŸ“ Environment Variables

Create `.env` file:
```env
DATABASE_URL="file:./dev.db"
NEXT_PUBLIC_APP_URL="http://localhost:3000"
SCRAPER_SECRET="your-secret-key"  # Optional
```

## ğŸ› Troubleshooting

### "Cannot find module @prisma/client"
```bash
npm install
npm run db:push
```

### Scraping returns no events
- Check website structure hasn't changed
- Update CSS selectors in `scraper.ts`
- Use mock events for testing

### Database locked error
```bash
# Stop all processes using database
pkill -f prisma
rm prisma/dev.db
npm run db:push
```

## ğŸ“š Tech Stack

- **Frontend**: Next.js 14, React 18, Tailwind CSS
- **Backend**: Next.js API Routes
- **Database**: Prisma ORM with SQLite (dev) / PostgreSQL (prod)
- **Scraping**: Axios, Cheerio
- **Styling**: Tailwind CSS, Lucide Icons
- **Automation**: node-cron

## ğŸ“ Learning Resources

- [Next.js Documentation](https://nextjs.org/docs)
- [Prisma Documentation](https://www.prisma.io/docs)
- [Tailwind CSS](https://tailwindcss.com/docs)
- [Web Scraping Guide](https://www.scrapingbee.com/blog/web-scraping-101/)

## ğŸ“„ License

MIT License - feel free to use for your projects!

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open pull request

## ğŸ“§ Support

For issues or questions, create an issue in the repository.

---

**Built for Assignment 1 - Sydney Events Aggregator**
